{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/safaabuzaid/segmentation-prompt-generator/blob/main/Prompt_driven.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg6g17nMeWDY"
      },
      "source": [
        "# **Prompt Generator for Radiology Segmentation tasks from Synthetic Clinical Notes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etuNFMQtg5nu"
      },
      "source": [
        "**Note:** This dataset is synthetically generated using ChatGPT for educational and demonstration purposes only. It does not represent real patient data and should not be used for clinical decision-making or real-world applications.  \n",
        "The goal is to create a prompt generator that can turn clinical notes into precise prompt that can be used later for segmentation tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KNU05yQeBee"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/clinical_notes.csv')\n",
        "df.info()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_nOMY6WeUnS"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVxUjhMxh43m"
      },
      "outputs": [],
      "source": [
        "#format the dataset for the model\n",
        "input_text = \"Clinical Note: [note]\"\n",
        "target_text = \"Prompt: [prompt]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pM5oYY3GvR_v"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "#create dictionary of note,prpompt\n",
        "data_dict = {'note': df['note'], 'prompt': df['prompt']}\n",
        "\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GY3zx2UW4--f"
      },
      "outputs": [],
      "source": [
        "print (df['note'][0])\n",
        "print (df['prompt'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "CT scan reveals a 3.2 cm irregular mass in the upper lobe of the left lung; biopsy confirms stage II adenocarcinoma.\n",
        "Segment tumor in left lung based on stage II adenocarcinoma\n"
      ],
      "metadata": {
        "id": "gtHrandx6nFL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqQsxJB4v_UJ"
      },
      "source": [
        "# Preprocessing the data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGNSTo5oj-vi"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 128\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"Generate a segmentation prompt from the following Clinical Note: \" + note for note in examples[\"note\"]]\n",
        "    targets = [\"Prompt: \" + prompt for prompt in examples[\"prompt\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, padding = \"max_length\", truncation=True, max_length=max_input_length)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "      labels = tokenizer(targets, padding = \"max_length\", truncation=True, max_length=max_target_length)\n",
        "\n",
        "    #targets= tokenizer(examples[\"prompt\"], padding = \"max_length\", truncation=True, max_length=max_target_length)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "\n",
        "```\n",
        "DatasetDict({\n",
        "    train: Dataset({\n",
        "        features: ['note', 'prompt', 'input_ids', 'attention_mask', 'labels'],\n",
        "        num_rows: 12\n",
        "    })\n",
        "    test: Dataset({\n",
        "        features: ['note', 'prompt', 'input_ids', 'attention_mask', 'labels'],\n",
        "        num_rows: 3\n",
        "    })\n",
        "})\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "TCzmcRSx6aOn"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFb-rdw0B9R"
      },
      "source": [
        "# Load The moodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIEgVBnnk1Pl"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezn8MlNY0IiN"
      },
      "source": [
        "# Set training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_NXRoRi3nyZm"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    report_to=None,\n",
        "    output_dir=\"./finetuned-flan-t5\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    num_train_epochs=5,\n",
        "    predict_with_generate=True,\n",
        "    fp16=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNBb989s1H2K"
      },
      "source": [
        "# Fine Tuning The model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgSleKnxr8xu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from transformers import Seq2SeqTrainer , DataCollatorForSeq2Seq\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwWtqGVE1kPF"
      },
      "outputs": [],
      "source": [
        "#test it to the dataset\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhjuAXyt5NNW"
      },
      "outputs": [],
      "source": [
        "input_text = \"Generate a segmentation prompt from the following Clinical Note: \" + df['note'][0]\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True,padding = \"max_length\", max_length = 512).to(model.device)\n",
        "\n",
        "generated_ids= model.generate(**inputs, max_new_tokens=50, num_beams = 4, early_stopping = True)\n",
        "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "\n",
        "if generated_text.lower().startswith(\"prompt:\"):\n",
        "  generated_text = generated_text[7:].strip()\n",
        "\n",
        "print (input_text)\n",
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output:\n",
        "\n",
        "Generate a segmentation prompt from the following Clinical Note: CT scan reveals a 3.2 cm irregular mass in the upper lobe of the left lung; biopsy confirms stage II adenocarcinoma.\n",
        "adenocarcinoma"
      ],
      "metadata": {
        "id": "iSu-FKLi6SHT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q-9iH_05rCN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPLkHueTYfLyFhrvXXiLyqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}